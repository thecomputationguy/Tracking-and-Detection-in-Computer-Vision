{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "trainingBatches = np.load(\"dataBatch.npy\") # Training Data\n",
    "batchSize, height, width, channels = trainingBatches.shape # Shapes of training batches\n",
    "\n",
    "# Some code to set up a log folder\n",
    "present_time = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, present_time)\n",
    "\n",
    "print(trainingBatches.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Build Phase\n",
    "with tf.name_scope(\"CNN\"):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, height, width, channels), name=\"Input\") # Input batch of images\n",
    "    conv1 = tf.layers.conv2d(inputs=X, filters=16, kernel_size=[8, 8], activation=tf.nn.relu, name=\"Conv1\")\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, name=\"Pool1\")\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, filters=16, kernel_size=[5, 5], activation=tf.nn.relu, name=\"Conv2\")\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, name=\"Pool2\")\n",
    "    pool2_flat = tf.layers.Flatten()(pool2)\n",
    "    fc1 = tf.layers.dense(inputs=pool2_flat, units=256, activation=tf.nn.relu, name=\"FullyConnected1\")\n",
    "    feats = tf.layers.dense(inputs=fc1, units=16, name=\"Features\")\n",
    "    \n",
    "with tf.name_scope(\"TripletLoss\"): # Triplet loss function\n",
    "    lossTriples = 0.0\n",
    "    lossPairs = 0.0\n",
    "    \n",
    "    diff_pos = feats[0:batchSize:3] - feats[1:batchSize:3]\n",
    "    diff_neg = feats[0:batchSize:3] - feats[2:batchSize:3]\n",
    "    lossTriples = tf.reduce_sum(tf.maximum(0.0 , 1.0 - ((diff_neg * diff_neg) / (diff_pos * diff_pos + 0.01))))\n",
    "    lossPairs = tf.reduce_sum(diff_pos * diff_pos)\n",
    "   \n",
    "    # Calculate Loss\n",
    "    lossCombined = lossTriples + lossPairs\n",
    "    \n",
    "with tf.name_scope(\"Optimizer\"): # Optimizer \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    training_op = optimizer.minimize(lossCombined)\n",
    "    \n",
    "loss_summary = tf.summary.scalar('Triplet_Loss', lossCombined) # To log the losses\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) # Write the losses to a file\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Step: 0 | Loss: 3061.54\n",
      "Epoch: 0 | Step: 50 | Loss: 82.8111\n",
      "Epoch: 0 | Step: 100 | Loss: 21.5896\n",
      "Epoch: 0 | Step: 150 | Loss: 13.1138\n",
      "Epoch: 0 | Step: 200 | Loss: 11.6786\n",
      "Epoch: 0 | Step: 250 | Loss: 8.33506\n",
      "Epoch: 0 | Step: 300 | Loss: 5.97838\n",
      "Epoch: 0 | Step: 350 | Loss: 6.65972\n",
      "Epoch: 0 | Step: 400 | Loss: 4.8618\n",
      "Epoch: 0 | Step: 450 | Loss: 4.41915\n",
      "Epoch: 0 | Step: 500 | Loss: 89.2198\n",
      "Epoch: 0 | Step: 550 | Loss: 9.28268\n",
      "Epoch: 0 | Step: 600 | Loss: 5.12985\n",
      "Epoch: 0 | Step: 650 | Loss: 3.52739\n",
      "Epoch: 0 | Step: 700 | Loss: 2.73131\n",
      "Epoch: 0 | Step: 750 | Loss: 2.07223\n",
      "Epoch: 0 | Step: 800 | Loss: 1.95864\n",
      "Epoch: 0 | Step: 850 | Loss: 1.79926\n",
      "Epoch: 0 | Step: 900 | Loss: 1.28874\n",
      "Epoch: 0 | Step: 950 | Loss: 1.12925\n",
      "Epoch: 0 | Step: 1000 | Loss: 1.16267\n",
      "Epoch: 0 | Step: 1050 | Loss: 0.968565\n",
      "Epoch: 0 | Step: 1100 | Loss: 0.937497\n",
      "Epoch: 0 | Step: 1150 | Loss: 1.0711\n"
     ]
    }
   ],
   "source": [
    "# Execution Phase\n",
    "\n",
    "n_epochs = 1\n",
    "batch_size = batchSize\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(batchSize):\n",
    "            _, loss = sess.run([training_op, lossCombined], feed_dict = {X: trainingBatches})\n",
    "            if (iteration % 50 == 0):\n",
    "                summary_str = loss_summary.eval(feed_dict = {X: trainingBatches})\n",
    "                step = (epoch + 1) * iteration\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "                print(\"Epoch:\", epoch, \"| Step:\", step, \"| Loss:\", loss)\n",
    "                save_path = saver.save(sess, \"./checkpoints/intermediate_model.ckpt\")\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = np.load(\"database_images_data.npy\")\n",
    "test = np.load(\"test_images_data.npy\")\n",
    "db_images = []\n",
    "test_images = []\n",
    "\n",
    "for img in test[0]:\n",
    "    test_images.append(img[0])\n",
    "\n",
    "for img in db[0]:\n",
    "    db_images.append(img[0])\n",
    "    \n",
    "test_images = np.asarray(test_images)\n",
    "db_images = np.asarray(db_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3540, 64, 64, 3)\n",
      "(1335, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "print(db_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/intermediate_model.ckpt\n",
      "(1335, 16)\n",
      "(3540, 16)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./checkpoints/intermediate_model.ckpt\")\n",
    "    dbDescriptor = feats.eval(feed_dict={X: db_images})\n",
    "    testDescriptor = feats.eval(feed_dict={X: test_images})    \n",
    "    \n",
    "print(dbDescriptor.shape)\n",
    "print(testDescriptor.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbDescriptorDatabase = []\n",
    "testDescriptorDatabase = []\n",
    "\n",
    "for index, testItem in enumerate(test[0]):\n",
    "    label = testItem[1]\n",
    "    descriptor = testDescriptor[index]\n",
    "    testDescriptorDatabase.append([descriptor, label])\n",
    "    \n",
    "for index, dbItem in enumerate(db[0]):\n",
    "    label = dbItem[1]\n",
    "    descriptor = dbDescriptor[index]\n",
    "    dbDescriptorDatabase.append([descriptor, label])\n",
    "    \n",
    "np.save(\"test_descriptors.npy\", testDescriptorDatabase)\n",
    "np.save(\"db_descriptors.npy\", dbDescriptorDatabase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
